{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay so nearly not all imports are being used by this notebook, but this gives a broad overview of the different models/sampling techniques that were tested\n",
    "\n",
    "# general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "\n",
    "# models\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, AdaBoostClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# evaluation \n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import average_precision_score, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, balanced_accuracy_score, classification_report, roc_auc_score, RocCurveDisplay, roc_curve, auc, precision_recall_curve, precision_score, PrecisionRecallDisplay\n",
    "\n",
    "# class imbalance\n",
    "from imblearn.under_sampling import NearMiss, RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler, SMOTENC\n",
    "from imblearn.ensemble import *\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import relplot as rp\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# saving the model\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_df(df: pd.DataFrame, training:bool=False) -> pd.DataFrame:\n",
    "    '''\n",
    "    Returns a dataframe with only the features.\n",
    "    It also sorts the dataframe in the right format/sorted\n",
    "    if training is set to true, the target variable will be added\n",
    "    '''\n",
    "    feature_cols = ['PATIENTNR', 'GESLACHT', 'LEEFTIJD', 'POSTCODE', 'WOONPLAATS', # patient features\n",
    "                'AGENDA', 'DESCRIPTION', 'CONSTYPE', 'CODE', 'SPECIALISME','LOCATIE', 'DUUR', # appointment features\n",
    "                'AfspraakZelfdeDag', 'AFSTAND', 'VerschilInplannenEnAfspraak', 'num_no_shows', 'perc_no_shows', 'stiptheid', 'MaandAfspraak', 'DagAfspraak', 'TijdAfspraak',  # engineerd features\n",
    "                'num_appointments', 'last_noshow', 'days_since_last_appointment', # engineerd features\n",
    "                ]\n",
    "    # if you want to train with the data, add target variable and date\n",
    "    if training:\n",
    "        feature_cols.append('STARTDATEPLAN')\n",
    "        feature_cols.append('no_show')\n",
    "\n",
    "    return df[feature_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILL_NA = False\n",
    "\n",
    "pd.set_option('display.max_columns', 400)\n",
    "df = pd.read_csv('/mnt/data/jmaathuis/no_shows/no_show_all_apps_run2_start_date=2020-01-01_hist=5_improved2.csv', parse_dates=['STARTDATEPLAN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_feature_df(df, training=True)\n",
    "df = df[~df['DagAfspraak'].isin(['Saturday', 'Sunday'])]  # not predicting appointments in the weekend\n",
    "df = df[df['CONSTYPE'].isin(['H', 'E', 'V', '*'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['STARTDATEPLAN'] >= '2022-01-01']\n",
    "\n",
    "if FILL_NA:\n",
    "    df.loc[df['AFSTAND'].isna(), 'AFSTAND'] = df['AFSTAND'].median()\n",
    "    df.loc[df['num_no_shows'].isna(), 'num_no_shows'] = df['num_no_shows'].mean()\n",
    "    df.loc[df['perc_no_shows'].isna(), 'perc_no_shows'] = df['perc_no_shows'].mean()\n",
    "    df.loc[df['stiptheid'].isna(), 'stiptheid'] = df['stiptheid'].mean()\n",
    "    df.loc[df['DUUR'].isna(), 'DUUR'] = df['DUUR'].mean()\n",
    "    df.loc[df['TijdAfspraak'].isna(), 'TijdAfspraak'] = df['TijdAfspraak'].value_counts().argmax()\n",
    "    df.loc[df['days_since_last_appointment'].isna(), 'days_since_last_appointment'] = df['days_since_last_appointment'].mean()\n",
    "\n",
    "\n",
    "cat_cols = ['GESLACHT', 'WOONPLAATS', 'POSTCODE', 'AGENDA', 'CONSTYPE', 'CODE', 'SPECIALISME', 'LOCATIE', 'DagAfspraak', 'DESCRIPTION', 'AfspraakZelfdeDag']\n",
    "\n",
    "df[cat_cols] = df[cat_cols].astype('category')\n",
    "if FILL_NA:\n",
    "    for col in df.select_dtypes(include='category'):\n",
    "        df[col] = df[col].cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No-Show distributions per specialism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_shows = pd.DataFrame(columns=['specialisme', 'n_appointments', 'n_no_shows', 'perc_no_shows'])\n",
    "counts = df['no_show'].value_counts()\n",
    "df_no_shows.loc[len(df_no_shows.index)] = ['TOTAAL', counts.sum(), counts[1], round(counts[1] / counts.sum() * 100, 2)]\n",
    "\n",
    "\n",
    "for spec in df['SPECIALISME'].unique():\n",
    "    counts = df[df['SPECIALISME'] == spec]['no_show'].value_counts()\n",
    "    if 1 in counts.keys():\n",
    "        df_no_shows.loc[len(df_no_shows.index)] = [spec, counts.sum(), counts[1], round(counts[1] / counts.sum() * 100, 2)]\n",
    "\n",
    "df_no_shows.sort_values(by='perc_no_shows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remvoing noisy specialisms (due to different reasons)\n",
    "spec_to_remove = ['RAD', 'APO', 'ONC', 'GEV', 'ORT', 'GGZ', 'PSY', 'ANE']\n",
    "df = df[~df['SPECIALISME'].isin(spec_to_remove)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train test split is based on year:   \n",
    "* The training set contains data from 2021 to 2023.   \n",
    "* Thest test set contains data from the years 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df.copy()\n",
    "\n",
    "df = df[df['STARTDATEPLAN'] >= pd.Timestamp(\"2020-01-01 00:00:00\")]\n",
    "df_test = df[df['STARTDATEPLAN'] > pd.Timestamp(\"2024-01-01 00:00:00\")]\n",
    "\n",
    "df = df[df['STARTDATEPLAN'] < pd.Timestamp(\"2024-01-01 00:00:00\")]\n",
    "df = df.drop(columns=['STARTDATEPLAN'])\n",
    "df_test = df_test.drop(columns=['STARTDATEPLAN', 'PATIENTNR'])\n",
    "\n",
    "X, y = df.drop(columns=['no_show', 'PATIENTNR']), df['no_show']\n",
    "X_test, y_test = df_test.drop(columns=['no_show']), df_test['no_show']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['no_show'].value_counts(dropna=False)\n",
    "df_all[df_all['STARTDATEPLAN'] > pd.Timestamp(\"2024-01-01 00:00:00\")]['no_show'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBClassifierResampled(XGBClassifier):\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        resampler = RandomOverSampler(sampling_strategy=0.4)\n",
    "        X, y = resampler.fit_resample(X, y)\n",
    "        super().fit(X, y, sample_weight=class_weight.compute_sample_weight('balanced',  y=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now this works fine, later run a grid/bayesian search\n",
    "classifier_params = {\n",
    "    'device':'cuda', \n",
    "    'enable_categorical':True,\n",
    "    'learning_rate':0.15, \n",
    "    'max_cat_to_onehot':25,\n",
    "    'max_cat_threshold':5,\n",
    "    'reg_alpha':10,\n",
    "    'scale_pos_weight':0.5,\n",
    "    'n_estimators': 350,\n",
    "    'importance_type': 'gain'\n",
    "}\n",
    "\n",
    "clf = XGBClassifier(**classifier_params)\n",
    "stratified_k_fold = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "metrics = ('precision', 'recall', 'roc_auc', 'f1', 'average_precision', 'balanced_accuracy')\n",
    "\n",
    "scores = cross_validate(clf, X, y, \n",
    "                        cv=stratified_k_fold, \n",
    "                        scoring=metrics, \n",
    "                        return_estimator=True,\n",
    "                        fit_params={'sample_weight': class_weight.compute_sample_weight('balanced',  y=y)}\n",
    "                        )\n",
    "\n",
    "for metric in ['f1', 'roc_auc', 'average_precision', 'balanced_accuracy']:\n",
    "    metric = 'test_' + metric   \n",
    "    print(f'{metric}: {scores[metric].mean():.3f} (+/-{np.std(scores[metric]):.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision recall plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing another single run to show the confusion matrix and PR-AUC\n",
    "X_train = X\n",
    "y_train = y\n",
    "\n",
    "clf = XGBClassifier(**classifier_params)\n",
    "\n",
    "clf.fit(X_train, y_train, sample_weight=class_weight.compute_sample_weight('balanced',  y=y_train))\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred_proba = clf.predict_proba(X_test)[:,1]\n",
    "# y_pred = (y_pred_proba > 0.6).astype(int)\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "print(roc_auc_score(y_test, y_pred_proba))\n",
    "print(average_precision_score(y_test, y_pred_proba))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Show', 'No-Show'])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.show()\n",
    "\n",
    "PrecisionRecallDisplay.from_predictions(y_test, y_pred_proba)\n",
    "plt.ylim(0, 0.7)\n",
    "plt.grid(alpha=.2)\n",
    "plt.xlim(0, 0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance per Specialism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plot = X_test.copy()\n",
    "\n",
    "y_pred_proba = clf.predict_proba(X_plot)[:,1]\n",
    "X_plot['y_score'] = y_pred_proba\n",
    "X_plot['y_true'] = y_test\n",
    "\n",
    "specs = []\n",
    "pr_aucs = []\n",
    "roc_aucs = []\n",
    "for spec in X_plot.SPECIALISME.unique():\n",
    "    X_plot_spec = X_plot[X_plot['SPECIALISME'] == spec]\n",
    "    if len(X_plot_spec) > 2000:\n",
    "        try:\n",
    "            pr_aucs.append(average_precision_score(X_plot_spec['y_true'], X_plot_spec['y_score']))\n",
    "            roc_aucs.append(roc_auc_score(X_plot_spec['y_true'], X_plot_spec['y_score']))\n",
    "            specs.append(spec)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "plt.title('PR-AUC')\n",
    "plt.barh(specs, pr_aucs)\n",
    "plt.show()\n",
    "\n",
    "plt.title('ROC-AUC')\n",
    "plt.barh(specs, roc_aucs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    'enable_categorical': [True],\n",
    "    'max_cat_to_onehot': (1, 50, 'uniform'),\n",
    "    'max_cat_threshold': (1, 10, 'uniform'),\n",
    "    'scale_pos_weight': (0, 1),\n",
    "    'reg_alpha': (0, 20, 'uniform'),\n",
    "    'reg_lambda': (0, 20, 'uniform'),\n",
    "    'n_estimators': (0, 1000, 'uniform'),\n",
    "    'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "    'grow_policy': ['depthwise', 'lossguide'],\n",
    "    'min_child_weight': (1, 10),\n",
    "    'subsample': (0.5, 1.0, 'uniform'),\n",
    "    'colsample_bytree': (0.5, 1.0, 'uniform'),\n",
    "    'colsample_bylevel': (0.5, 1.0, 'uniform'),\n",
    "    'colsample_bynode': (0.5, 1.0, 'uniform'),\n",
    "    'gamma': (0.01, 10.0, 'log-uniform'),\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize the Bayesian Optimization optimizer\n",
    "opt = BayesSearchCV(\n",
    "    estimator=XGBClassifier(),\n",
    "    search_spaces=param_space,\n",
    "    scoring='average_precision',  \n",
    "    n_iter=100,  \n",
    "    cv=5,  \n",
    ")\n",
    "\n",
    "# Run Bayesian Optimization\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "best_params = opt.best_params_\n",
    "best_score = opt.best_score_\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "final_model = XGBClassifier(**best_params)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the final model on a separate test dataset\n",
    "final_model_score = roc_auc_score(y_test, final_model.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = final_model\n",
    "\n",
    "feature_imps = {feature: importance for feature, importance in zip(X.columns, clf.feature_importances_)}\n",
    "feature_imps = dict(sorted(feature_imps.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(feature_imps.keys(), feature_imps.values(), color='C1', alpha=.5)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a full retrain on the whole dataset (train+test) so that we can use this for inference\n",
    "df_all = df_all.sample(frac=1)  # shuffles the dataset\n",
    "df_all = df_all.drop(columns=['STARTDATEPLAN', 'PATIENTNR'], axis=1)\n",
    "X, y = df_all.drop(columns=['no_show']), df_all['no_show']\n",
    "clf = XGBClassifier(**classifier_params)\n",
    "\n",
    "clf.fit(X, y, sample_weight=class_weight.compute_sample_weight('balanced',  y=y))\n",
    "\n",
    "\n",
    "joblib.dump(clf, '../5_Deployment/no_show_model_v2.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "4a8c2fb5217c5c27c23ac66d76c937205c5ea3ed670dcb9450dea28cbe2a6cff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
